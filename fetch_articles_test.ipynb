{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fetch_articles function for testing\n",
    "def fetch_articles(url, save_dir):\n",
    "    \"\"\"\n",
    "    Fetches articles from the specified URL and saves them in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to fetch articles from.\n",
    "        save_dir (str): The directory where the articles will be saved.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with article titles and content.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch articles. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    articles = soup.find_all(\"article\")\n",
    "    if not articles:\n",
    "        print(\"No articles found on the webpage.\")\n",
    "        return []\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    saved_articles = []\n",
    "    for i, article in enumerate(articles[:5]):  # Limit to 5 articles for testing\n",
    "        title_element = article.find(\"h2\")\n",
    "        title = title_element.get_text(strip=True) if title_element else f\"article_{i+1}\"\n",
    "\n",
    "        content_element = article.find(\"p\")\n",
    "        content = content_element.get_text(strip=True) if content_element else \"No content available.\"\n",
    "\n",
    "        filename = os.path.join(save_dir, f\"{title.replace(' ', '_')}.txt\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{title}\\n\\n{content}\")\n",
    "\n",
    "        print(f\"Saved: {filename}\")\n",
    "        saved_articles.append({\"title\": title, \"content\": content})\n",
    "\n",
    "    return saved_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/raw_test/Home-grown_and_global_problems.txt\n",
      "Fetched Articles:\n",
      "Home-grown and global problems - VW produces and sells vehicles worldwide. Its Germanness is an important selling point, but the company is equally at home in China, Brazil and the US. Its dependence on foreign markets may soon come to bite.\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "test_url = \"https://www.dw.com/en/volkswagen-vw-banking-on-global-sales-to-stay-ahead-of-the-mobility-curve/a-71064923\"  # Replace with a real URL\n",
    "test_save_dir = \"data/raw_test/\"\n",
    "\n",
    "# Run fetch_articles and display the result\n",
    "articles = fetch_articles(test_url, test_save_dir)\n",
    "print(\"Fetched Articles:\")\n",
    "for article in articles:\n",
    "    print(article[\"title\"], \"-\", article[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
